# Assignment 2

### Dataset description ###

The dataset used in this assignment (lrn14) is retrieved from a larger dataset of "international survey of Approaches to Learning" -study (learning2014), belonging to research project ASSIST 2014 by Kimmo Vehkalahti.

The original dataset includes the points the students (n=183) participating in the course "Introduction to Social Statistics" in fall 2014 (in Finnish) received in the course exam, and several variables derived from survey questions that determine the students' attitude towards statistics (based on SATS -questionnary), approaches and study skills (based on ASSIST-questionnary). Background variables include age and gender. More information about the original dataset is available at https://www.mv.helsinki.fi/home/kvehkala/JYTmooc/JYTOPKYS3-meta.txt.

The original data has many questions that measure the same *dimension*, that have been transformed to combination variables in the lrn14 dataset. These represent the mean values of all questions that measure deep, surface and strategic learning. Furthermore, the student's attitude towards statistics has been originally measured as a sum variable of ten Likert scaled (1-5) questions, and the lrn14 dataset includes the mean value of these. Finally, students that have not attended the course examp (points = 0) have been excluded.

To get started, the dataset is loaded to R using the `read.table` -function:
```{r}
lrn14 <- read.table("https://raw.githubusercontent.com/KimmoVehkalahti/Helsinki-Open-Data-Science/master/datasets/learning2014.txt", sep=",", header=TRUE) # The separator is a comma "," and the file includes a header
```

Then dataset's dimensions and structure can be assessed using `dim()`and `str()`:
```{r}
dim(lrn14)
str(lrn14)
```

The dataset has 166 rows and 7 columns (variables), including gender, age, attitude, deep, stra, surf and points.

### Graphical overview of the data ### 
```{r}
# Access the GGally and ggplot2 libraries

library(ggplot2)
library(GGally)

# A scatter plot matrix of the variables
# [-1] excludes the first column (gender); this needs to be excluded since the `pairs` works only with numerical variables
pairs(lrn14[-1])

# A more advanced plot matrix can be produced with ggpairs()
p <- ggpairs(lrn14, mapping = aes(col=gender, alpha = 0.3), lower = list(combo = wrap("facethist", bins = 20)))

# Print the matrix
p
```



The age distribution is quite similar between the two genders; majority of the students are in their early 20Â´s.

Based on the plots and correlation coefficient, there seems to be statistically significant positive correlation between the students' attitude and exam points (in both genders) and negative correlation between the deep and surface learning skills of male students.

### Creating a linear model ### 

Let's try and create a regression model that predicts the (future) students' exam points. 

The three most promising explanatory variables for such a regression model would be the variables attitude, stra and surf. Let's create a linear model using these variables:
```{r}
# fit a linear model
my_model <- lm(points ~ attitude + stra + surf, data = lrn14)

# print out a summary of the model

summary(my_model)

```

The exam points would not be predicted very accurately using this model, since the model only explains around 20 % of the outcome (=points) variation in the sample (Multiple R-squared:0.2074). Based on statistical significance (as in enough correlation with the outcome), only attitude seems worth keeping in the model. We could try to improve the model by extracting the other two explanatory variables from the model:

```{r}
# fit a linear model
my_model2 <- lm(points ~ attitude, data = lrn14)

# print out a summary of the model

summary(my_model2)

```

This model is not, however, much better, since the predictive ability (Multiple R-squared) is still only around 19 %. Let's plot the corresponding regression line:

```{r}

library(ggplot2)

p1 <- ggplot(lrn14, aes(x = attitude, y = points))

# define the visualization type (points)
p2 <- p1 + geom_point()

# draw the plot
p2

# add a regression line
p3 <- p2 + geom_smooth(method = "lm")

p3
```

### Diagnostic plots ###

Diagnostic plots are necessary in order to evaluate that the four assumptions of a linear model fulfill. The assumptions are:

1. Linear relationship between the predictor(s) and outcome (if it is not linear, than a linear model most likely doesn't fit very well);
2. Independence of residuals, (that is, one student's points do not affect another student's points); 
3. Normal distribution of residuals (exam points should be normally distributed); and
4. Equal variance of residuals (there is no greater variance among students with appalling exam points than those with good points)

The linear relationship here seems valid, based on the previous plot.

The rest will be evaluated using the following diagnostic plots: Residuals vs Fitted values, Normal QQ-plot and Residuals vs Leverage.

```{r}

# par(mfrow = c(2,2)) will place the following 4 graphics to the same plot

par(mfrow = c(2,2))

# We use the "plot()" function and specifically choose plots 1, 2 and 5 using the argument `which`. You can check the help page of plotting an lm object by typing `?plot.lm` or `help(plot.lm)` to the R console. 

plot(my_model2, which = c(1,2,5))
```

The residuals seem to be quite normally distributed, as seen in Residuals vs Fitted. The residuals lie quite randomly above and below zero. The residuals seem to be of similar size with lower and higher fitted values; however, there are some outliers that bounce up, also violating the normal distribution of the residuals (the same outliers are seen on the The Normal Q-Q plot that is otherwise quite satisfactory).

The residuals vs leverage gives information about the data points that may distort the model. The students number 35, 56 and 71 have large residuals, so removing these students from the dataset would perhaps add to linear model accuracy.


```{r}
date()
```

