# Assignment 3

### Dataset description ###

The analysis dataset used in this assignment is based on "Student Performance Data", available at the UCI Machine Learning Repository and kindly shared with us by Paulo Cortez.

The Student Performance Data includes _two_ datasets regarding student performance in secondary education of two Portuguese schools, in two distinct subjects: mathematics and portuguese language. In addition, several background variables, such as age, sex, geopraphical and family history, are included. The "Student Performance Data" dataset and metadata are available [here](https://archive.ics.uci.edu/ml/datasets/Student+Performance).

For this assigment, data is unified from these two datasets via several background variables. Thus only students who answered the questionnaire in both math and portuguese classes are kept in our analysis dataset. Duplicated data has been removed by keeping the first entry for character variables. In case of numerical variables, it has been replaced with the mean.

```{r}
library(readr)
alc <- read_csv("https://raw.githubusercontent.com/KimmoVehkalahti/Helsinki-Open-Data-Science/master/datasets/alc.csv", show_col_types=F)

# Print out all column names
colnames(alc)
```

New variables have been introduced to the analysis dataset: Variable "alc_use" is the average alcohol consumption in week, calculated from "Dalc" (workday alcohol consumption (numeric: from 1 - very low to 5 - very high)) and "Walc" (weekend alcohol consumption (numeric: from 1 - very low to 5 - very high)). Variable "alc_high" is TRUE, when alc_use > 2.

### Data analysis ###

The purpose of this weeks analysis is to study the relationships between high/low alcohol consumption and some of the other variables in the data. Since the outcome variable is binary, we shall use logistic regression with binomial (Bernoulli) distribution. Basically, we will be comparing the observed relationships in the model to the odds of a coin toss.

To do this, we first choose 4 interesting variables in the data. Let's start by drawing a bar plot of each variable to get a better conseption of the variables and their distribution.

```{r}

library(tidyr); library(dplyr); library(ggplot2)

gather(alc) %>% ggplot(aes(value)) + facet_wrap("key", scales = "free") + 
  geom_bar()

```



Regarding important background variables; sex usually represents a covariate, so it probably should be included in the model.

```{r}
library(dplyr); library(ggplot2)

# Summary statistics by group

alc %>% group_by(sex, high_use) %>% summarise(count = n())
p1 <- table(alc$sex, alc$high_use)
prop.table(p1)
```



There seems to be higher alcohol consumption among males than females, so 1. sex. Another important backgroud variable might be age (2.), as perhaps the youngest students may not be allowed to purchase alcohol themselves.

Regarding the other variables, alcohol consumption might - based on previous research data and personal experience - affect grades (3.), cause more absences (4.) and failures in exams (5.). These students may also go out more, as alcohol is often consumed while partying (6.). High alcohol consumtion might also deteriorate health (6.).

Let's plot these variables against high_use, separating females and males. Since the grade has three repetative measures / one individual (G1, G2 and G3), we will only include one of these, the final exam (G3). 

```{r}
library(tidyr); library(dplyr); library(ggplot2)

g1 <- ggplot(alc, aes(x = high_use, y = age, col=sex))
g1 + geom_boxplot() + ylab("age")

g2 <- ggplot(alc, aes(x = high_use, y = G3, col=sex))
g2 + geom_boxplot() + ylab("grade")

g3 <- ggplot(alc, aes(x = high_use, y = absences, col=sex)) 
g3 + geom_boxplot() + ylab("absences")

g4 <- ggplot(alc, aes(x = high_use, y = failures, col=sex)) 
g4 + geom_boxplot() + ylab("failures")

g5 <- ggplot(alc, aes(x = high_use, y = goout, col=sex)) 
g5 + geom_boxplot() + ylab("go out")

g6 <- ggplot(alc, aes(x = high_use, y = health, col=sex)) 
g6 + geom_boxplot() + ylab("health")

```



The median age of male students with high alcohol consumption is higher than of those with low alcohol consumption, and the opposite is true for females. However, the outlier student that is 22 years old within male high alcohol consumer group may distort the distribution. The overall age distribution is also quite narrow (majority of the students are about the same age) which may weaken the variable's predictive ability.

Male students with high alcohol consumption seem to have lower grades (again, there are outliers), and increased absences. There also seems to be more going out among high alcohol consumers, regardless of sex. All of these variables thus seem promising for the model.

Surprisingly, female students with high alcohol consumption report better state of health than those with low alcohol consumption! We could include this variable in the model, but the pre-assumption here would suggest the opposite, so the results gained might not be trustworthy. 

Regarding failures, majority of the cases in the dataset have passed their exams (median = 0), so the boxplot is therefore not very informative. We'll have to try it out.

Let's start with failures + absences + sex + G3 (model 1). 

### Logistic regression ###


```{r}
m1 <- glm(high_use ~ failures + absences + sex + G3, data = alc, family = "binomial")
summary(m1)
```



The deviance tells us how well our model fits the data, in terms of error. The coefficient estimates are the _logarithmic odds_ of change in the response variable, in respect to one unit change in the explanatory variable, keeping that the other variables stay put (conditionally on these). Logartimic link function is necessary here to provide equal scale to the response variable that varies from 0-100% (or 0-1) and the explanatory variables (with also numerical values included). 

The interpretation regarding sex here is a bit different: one unit change in this variable means transition from one category to other; from female (0) to male (1). 

The odds can be tabulated with exp(Estimate) = propability/(1-probability) = ODDS. These can be interpreted as _odds ratios between one unit change vs. no change_ in the corresponding explanatory variable.

Failures are significant; and absences and sex highly significant predictors of the probability of being judged to have high alcohol consumption in this model. Grades are not.

There is also AIC = Akaike information criterion, that tells us how economic the model is. That is, how good is the predictive ability when considering the number of variables included, as too many might cause overfitting.

Next, let's change grades to "go out".

```{r}
m2 <- glm(high_use ~ failures + absences + sex + goout, data = alc, family = "binomial")
summary(m2)
```


With model 2, AIC is lower (which is good!) and all the variables are statistically significant. Then we should check for multicollinearity:

```{r}
library(car)
vif(m2)
```


VIF is low for all the variables, so no worries in that regard. Then, let's calculate the corresponding ODDS ratios and confidence intervals (95%):

```{r}
OR <- coef(m2) %>% exp
CI <- confint(m2) %>% exp
cbind(OR, CI)
```



The odds ratio for having high alcohol consumption is 1.6 if failures increase by one (unit); 1.1 if absences increase by one (unit); 2.7 if the student is a male (compared to being female); and 2.0 if going out increases by one (unit). These are conditional of the other variable values staying similar.

If the 95% CI would cross the value "1", it would mean that the true odds ratio may be 1/1 = no difference in the odds per one unit change in the particular explanatory variable.

The pre-stated hypothesis for a relationship between failures, absences, going out (and male gender, as an important background variable) and high alcohol consumption seems valid.

### Predicting exercise ###

```{r}
library(dplyr)
# predict the probability of high_use
probabilities <- predict(m2, type = "response")
alc <- mutate(alc, probability = probabilities)
alc <- mutate(alc, prediction = probability>0.5)
table(high_use = alc$high_use, prediction = alc$prediction)

```



Graphic visualizing both the actual values and the predictions:
```{r}
library(dplyr); library(ggplot2)

g <- ggplot(alc, aes(x = probability, y = high_use, col = prediction))
g + geom_point()

```



This visualizes that the model performs better on low alcohol users than among high alcohol users.

Training error using mean prediction error:
```{r}

loss_func <- function(class, prob) {
  n_wrong <- abs(class - prob) > 0.5
  mean(n_wrong)
}

lf <- loss_func(class = alc$high_use, prob = alc$probability)
1-lf 
```



79 % of the model's predictions would be correct (21 % would be incorrect). If I was to toss a coin (for a thousand times) guessing "heads" every time, I would be right in 50 % of cases. So the model exceeds pure guess.


### Cross validation ###

Let's perform 10-fold cross-validation on the model and compare the test set performance (measured as smaller prediction error) to the model introduced in the Exercise Set (which had about 0.26 error). 

```{r}
library(boot)
cv <- cv.glm(data = alc, cost = loss_func, glmfit = m2, K = 10)

# average number of wrong predictions in the cross validation
cv$delta[1]
```



This model's test set performance exceeds the performance of the Exercise set model.

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}

date()
```




